{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river = \"Anabar\"\n",
    "river2 = river.replace(\" \", \"_\")\n",
    "\n",
    "path_data0 = os.getcwd() + \"\\\\\"\n",
    "path_data1 = path_data0 + r\"ERA5_Land\\\\\"\n",
    "path_data2 = path_data0 + r\"Data\\\\\"\n",
    "path_df = r\"Table\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list1 = [\n",
    "    \"dewpoint_temperature_2m\",\n",
    "    \"temperature_2m\",\n",
    "    \"soil_temperature_level_1\",\n",
    "    \"snow_depth_water_equivalent\",\n",
    "    \"snowfall_sum\",\n",
    "    \"snowmelt_sum\",\n",
    "    \"sub_surface_runoff_sum\",\n",
    "    \"surface_runoff_sum\",\n",
    "    \"surface_net_solar_radiation_sum\",\n",
    "    \"surface_net_thermal_radiation_sum\",\n",
    "    \"total_evaporation_sum\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"u_component_of_wind_10m\",\n",
    "    \"v_component_of_wind_10m\",\n",
    "]\n",
    "\n",
    "var_list2 = [\n",
    "    \"Dewpoint_temperature_2m\",\n",
    "    \"Temperature_2m\",\n",
    "    \"Soil_temperature_1\",\n",
    "    \"Snow_depth_water_equivalent\",\n",
    "    \"Snowfall\",\n",
    "    \"Snowmelt\",\n",
    "    \"Sub_surface_runoff\",\n",
    "    \"Surface_runoff\",\n",
    "    \"Surface_net_solar_radiation\",\n",
    "    \"Surface_net_thermal_radiation\",\n",
    "    \"Evaporation\",\n",
    "    \"Precipitation\",\n",
    "    \"Wind_U\",\n",
    "    \"Wind_V\",\n",
    "]\n",
    "\n",
    "prefix_list = [\"p1\", \"basin\"]\n",
    "\n",
    "dem_list = [\"elevation\", \"slope\", \"aspect\"]\n",
    "dem_list2 = [f\"{p}_{d}\" for p in prefix_list for d in dem_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [2, 15, 30, 45]  # Number of days to calculate the moving average\n",
    "\n",
    "date1 = pd.Timestamp(1950, 1, 1)\n",
    "date2 = pd.Timestamp(2023, 12, 31)\n",
    "time_index = pd.date_range(start=date1, end=date2)\n",
    "\n",
    "date_missing = pd.Timestamp(1950, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_era5(df, prefix):\n",
    "    df[f\"{prefix}_Wind\"] = np.sqrt(\n",
    "        df[f\"{prefix}_Wind_U\"] ** 2 + df[f\"{prefix}_Wind_V\"] ** 2\n",
    "    )\n",
    "    df.drop(columns=[f\"{prefix}_Wind_U\", f\"{prefix}_Wind_V\"], inplace=True)\n",
    "\n",
    "    # fill the missing values for Dec 31 with backward fill\n",
    "    d1 = df.index.min()\n",
    "    d2 = df.index.max()\n",
    "    time_index0 = pd.date_range(start=d1, end=d2, freq=\"D\")\n",
    "    df = df.reindex(time_index0, method=\"ffill\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_date_to_sin_cos(dates):\n",
    "    if not isinstance(dates, pd.Series):\n",
    "        raise ValueError(\"输入必须是pandas Series\")\n",
    "\n",
    "    dates = pd.to_datetime(dates)\n",
    "\n",
    "    is_leap_year = dates.dt.is_leap_year\n",
    "    day_of_year = dates.dt.dayofyear\n",
    "    days_in_year = np.where(is_leap_year, 366, 365)\n",
    "\n",
    "    JD_sin = np.sin(day_of_year * 2 * np.pi / days_in_year)\n",
    "    JD_cos = np.cos(day_of_year * 2 * np.pi / days_in_year)\n",
    "\n",
    "    return pd.Series(JD_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_station = \"Station Info5.csv\"\n",
    "df_station = pd.read_csv(file_station)\n",
    "df_station = df_station[df_station[\"River\"] == river]\n",
    "assert len(df_station) == 1\n",
    "station = df_station[\"Station\"].values[0]\n",
    "station2 = station.replace(\" \", \"_\").replace(\",\", \"\").replace(\".\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "file_discharge = path_data2 + f\"{river}__{station}.csv\"\n",
    "df2 = pd.read_csv(file_discharge, parse_dates=[\"Date\"], index_col=\"Date\", usecols=[\"Date\", \"Discharge\"])\n",
    "df2[\"Discharge\"] = df2[\"Discharge\"].clip(lower=0)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = pd.read_csv(path_data1 + f\"{river2}__{station2}__Point_DEM.csv\").drop(columns=[\"system:index\", \".geo\"]).squeeze()\n",
    "df_temp2 = pd.read_csv(path_data1 + f\"{river2}__{station2}__Basin_DEM.csv\").drop(columns=[\"system:index\", \".geo\"]).squeeze()\n",
    "df_dem = pd.concat([df_temp1, df_temp2], axis=0)\n",
    "\n",
    "df_temp3 = pd.read_csv(path_data1 + f\"{river2}__{station2}__Point_ERA5_Land.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "df_temp4 = pd.read_csv(path_data1 + f\"{river2}__{station2}__Basin_ERA5_Land.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "\n",
    "df1 = pd.concat([df_temp3, df_temp4], axis=1)\n",
    "df1.index.names = [\"Date\"]\n",
    "df1.drop(columns=[\"system:index\", \".geo\"], inplace=True)\n",
    "if date1 <= date_missing <= date2:\n",
    "    # fill the values of \"1950-01-01\" with \"1950-01-02\"\n",
    "    df1.loc[date_missing] = np.nan\n",
    "    df1.sort_index(inplace=True)\n",
    "    df1.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "for prefix in prefix_list:\n",
    "    col_list1 = [f\"{prefix}_{col}\" for col in var_list1]\n",
    "    col_list2 = [f\"{prefix}_{col}\" for col in var_list2]\n",
    "    df1.rename(columns=dict(zip(col_list1, col_list2)), inplace=True)\n",
    "    df1 = preprocess_era5(df1, prefix)\n",
    "\n",
    "enviro_vars = df1.columns.tolist()\n",
    "\n",
    "for dem in df_dem.index:\n",
    "    df1[dem] = df_dem[dem]\n",
    "    \n",
    "df1[\"JD\"] = df1.index.dayofyear\n",
    "df1[\"JD_cos\"] = encode_date_to_sin_cos(df1.index.to_series())\n",
    "\n",
    "cols = df1.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "df1 = df1[cols]\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df1, df2], axis=1)\n",
    "df3 = df3.resample(\"D\").mean()\n",
    "\n",
    "df3.insert(0, \"River\", river)\n",
    "df3.insert(1, \"Station\", station)\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean\n",
    "var_list3 = df3.columns.drop(\n",
    "    [\"River\", \"Station\", \"Discharge\", \"JD\", \"JD_cos\"] + df_dem.index.tolist()\n",
    ").tolist()\n",
    "\n",
    "for enviro_var in enviro_vars:\n",
    "    for ws in window_sizes:\n",
    "        df3[f\"{enviro_var}__{ws}\"] = df3[enviro_var].rolling(window=ws, min_periods=1).mean()\n",
    "\n",
    "df3 = df3.copy()\n",
    "df3 = df3.reindex(time_index)\n",
    "df3.index.name = \"Date\"\n",
    "\n",
    "# Drop not rolling mean columns\n",
    "df3 = df3.drop(columns=var_list3)\n",
    "print(df3.shape)\n",
    "df3.to_csv(path_df + \"Data0_with_DEM.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df3.columns:\n",
    "    for col2 in dem_list:\n",
    "        if col2 in col:\n",
    "            df3.drop(columns=col, inplace=True)\n",
    "\n",
    "df3.to_csv(path_df + \"Data1_with_nan.csv\", index=True)\n",
    "print(df3.shape)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.dropna(how=\"any\")\n",
    "df4.to_csv(path_df + \"Data2_without_nan.csv\", index=True)\n",
    "print(df4.shape)\n",
    "\n",
    "ratio = df4.shape[0] / df3.shape[0] * 100\n",
    "print(\"Ratio of non-NaN data: {:.2f}%\".format(ratio))\n",
    "\n",
    "if df3.shape[0] == df4.shape[0]:\n",
    "    print(\"No NaN data in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
