{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = r\"Table\" + \"\\\\\"\n",
    "path_model = r\"Model\" + \"\\\\\"\n",
    "path_fig = r\"Figure\" + \"\\\\\"\n",
    "\n",
    "col_list = [\"River\", \"Station\", \"Discharge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", font=\"Times New Roman\")\n",
    "\n",
    "font1 = {\"family\": \"Times New Roman\", \"weight\": \"bold\", \"size\": 16}\n",
    "\n",
    "sources = [\"Observed\", \"Predicted\", \"Interpolated\", \"Boundary matched\"]\n",
    "colors = [\"blue\", \"green\", \"yellow\", \"red\"]\n",
    "color_dict = dict(zip(sources, colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model_performance = pd.read_csv(path_df + \"Model_performance.csv\")\n",
    "river = file_model_performance[\"River\"].values[0]\n",
    "\n",
    "best_model = file_model_performance[file_model_performance[\"Best Model\"] == 1][\n",
    "    \"Model\"\n",
    "].values[0]\n",
    "\n",
    "\n",
    "file_best_paras = path_model + best_model + \"_best_params.json\"\n",
    "with open(file_best_paras, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "if best_model == \"RF\":\n",
    "    model = RandomForestRegressor(**best_params)\n",
    "elif best_model == \"XGB\":\n",
    "    model = XGBRegressor(**best_params)\n",
    "else:\n",
    "    raise ValueError(\"No such model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_features = path_df + f\"Selected_features_{best_model}.csv\"\n",
    "features = pd.read_csv(file_features).iloc[:, 0].tolist()\n",
    "\n",
    "df = pd.read_csv(path_df + \"Data1_with_nan.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropna = df.dropna(subset=features + col_list)\n",
    "\n",
    "X_train = df_dropna[features]\n",
    "y_train = df_dropna[\"Discharge\"]\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df to nan and non-nan\n",
    "df1 = df[df.isnull().any(axis=1)]\n",
    "df2 = df.dropna()[col_list]\n",
    "df2[\"Source\"] = \"Observed\"\n",
    "\n",
    "# Predict nan values\n",
    "df1[\"Discharge\"] = model.predict(df1[features])\n",
    "df1[\"Discharge\"] = df1[\"Discharge\"].clip(lower=0)\n",
    "df1 = df1[col_list]\n",
    "df1[\"Source\"] = \"Predicted\"\n",
    "\n",
    "# Combine df1 and df2\n",
    "df = pd.concat([df1, df2]).sort_index()\n",
    "df[\"Discharge\"] = df[\"Discharge\"].round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing for Temporal Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge = df[\"Discharge\"].values\n",
    "source = df[\"Source\"].values\n",
    "dates = df.index\n",
    "\n",
    "observed_indices = np.where(source == \"Observed\")[0]\n",
    "predicted_indices = np.where(source == \"Predicted\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_segments(predicted_indices):\n",
    "    segments = []\n",
    "    start = predicted_indices[0]\n",
    "    for i in range(1, len(predicted_indices)):\n",
    "        if predicted_indices[i] != predicted_indices[i - 1] + 1:\n",
    "            segments.append((start, predicted_indices[i - 1]))\n",
    "            start = predicted_indices[i]\n",
    "    segments.append((start, predicted_indices[-1]))\n",
    "    return segments\n",
    "\n",
    "\n",
    "missing_segments = find_missing_segments(predicted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for optimization (boundary matching)\n",
    "def objective_function(\n",
    "    adjustment_factors,\n",
    "    discharge,\n",
    "    predicted_indices,\n",
    "    boundary_obs_pred,\n",
    "    boundary_pred_obs,\n",
    "):\n",
    "    adjusted_discharge = discharge.copy()\n",
    "    adjusted_discharge[predicted_indices] *= adjustment_factors\n",
    "\n",
    "    boundary_mismatch = 0\n",
    "\n",
    "    # Check if boundary_obs_pred is not empty and indices are valid\n",
    "    if len(boundary_obs_pred) > 0 and np.all(boundary_obs_pred + 1 < len(discharge)):\n",
    "        boundary_mismatch += np.sum(\n",
    "            (discharge[boundary_obs_pred] - adjusted_discharge[boundary_obs_pred + 1])\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "    # Check if boundary_pred_obs is not empty and indices are valid\n",
    "    if len(boundary_pred_obs) > 0 and np.all(boundary_pred_obs + 1 < len(discharge)):\n",
    "        boundary_mismatch += np.sum(\n",
    "            (adjusted_discharge[boundary_pred_obs] - discharge[boundary_pred_obs + 1])\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "    return boundary_mismatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each missing segment\n",
    "for start_idx, end_idx in missing_segments:\n",
    "    segment_length = end_idx - start_idx + 1\n",
    "    segment_dates = dates[start_idx:end_idx + 1]\n",
    "\n",
    "    # Condition (1): Length does not exceed 20 days and does not contain data from May to July\n",
    "    if segment_length <= 20 and not ((segment_dates.month >= 5) & (segment_dates.month <= 7)).any():\n",
    "        # Check boundaries to avoid index out of range\n",
    "        if start_idx > 0 and end_idx < len(discharge) - 1:\n",
    "            # Linear interpolation\n",
    "            interp_func = interp1d([start_idx - 1, end_idx + 1], [discharge[start_idx - 1], discharge[end_idx + 1]], kind='linear')\n",
    "            discharge[start_idx:end_idx + 1] = interp_func(np.arange(start_idx, end_idx + 1))\n",
    "            source[start_idx:end_idx + 1] = \"Interpolated\"\n",
    "        else:\n",
    "            # Handle missing segments at the beginning or end of the sequence\n",
    "            if start_idx == 0:\n",
    "                # If the segment is at the start of the sequence, fill with the right observation value\n",
    "                discharge[start_idx:end_idx + 1] = discharge[end_idx + 1]\n",
    "            elif end_idx == len(discharge) - 1:\n",
    "                # If the segment is at the end of the sequence, fill with the left observation value\n",
    "                discharge[start_idx:end_idx + 1] = discharge[start_idx - 1]\n",
    "            source[start_idx:end_idx + 1] = \"Interpolated\"\n",
    "\n",
    "    else:\n",
    "        # Condition (2): Boundary condition matching\n",
    "        # Identify predicted values for the current segment\n",
    "        predicted_segment_indices = np.arange(start_idx, end_idx + 1)\n",
    "        boundary_obs_pred = np.array([start_idx - 1] if start_idx - 1 in observed_indices else [])\n",
    "        boundary_pred_obs = np.array([end_idx + 1] if end_idx + 1 in observed_indices else [])\n",
    "\n",
    "        # Initialize adjustment factors\n",
    "        initial_factors = np.ones(len(predicted_segment_indices))\n",
    "\n",
    "        # Optimize adjustment factors\n",
    "        result = minimize(\n",
    "            objective_function,\n",
    "            initial_factors,\n",
    "            args=(discharge, predicted_segment_indices, boundary_obs_pred, boundary_pred_obs),\n",
    "            method=\"L-BFGS-B\",\n",
    "            bounds=[(0.2, 4)] * len(predicted_segment_indices)\n",
    "        )\n",
    "\n",
    "        # Compute X days: 10% of the segment length or 10 days, whichever is smaller\n",
    "        X_days = min(int(segment_length * 0.1), 10)\n",
    "        X_days = max(X_days, 3)  # Ensure X_days is at least 3\n",
    "        X_days = min(X_days, segment_length)  # Ensure X_days does not exceed the segment length\n",
    "\n",
    "        # Check if there are observed data before the segment and adjust the first X days\n",
    "        if len(boundary_obs_pred) > 0 and X_days > 0:\n",
    "            discharge[start_idx:start_idx + X_days] *= result.x[:X_days]\n",
    "            source[start_idx:start_idx + X_days] = \"Boundary matched\"\n",
    "\n",
    "        # Check if there are observed data after the segment and adjust the last X days\n",
    "        if len(boundary_pred_obs) > 0 and X_days > 0:\n",
    "            discharge[end_idx - X_days + 1:end_idx + 1] *= result.x[-X_days:]\n",
    "            source[end_idx - X_days + 1:end_idx + 1] = \"Boundary matched\"\n",
    "\n",
    "# Save the adjusted results back to the DataFrame\n",
    "df[\"Source\"] = source\n",
    "\n",
    "df.to_csv(path_df + \"Data3_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.Timestamp(1950, 1, 1)\n",
    "date2 = pd.Timestamp(2025, 1, 1)\n",
    "date_list = pd.date_range(date1, date2, freq=\"5YS\").strftime(\"%Y\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "actual_sources = df[\"Source\"].unique()\n",
    "for source in sources:\n",
    "    if source in actual_sources:\n",
    "        df_source = df[df[\"Source\"] == source]\n",
    "        df_source = df_source.reindex(dates)\n",
    "\n",
    "        ax.plot(\n",
    "            df_source.index, df_source[\"Discharge\"], label=source, color=color_dict[source]\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Time\", font=font1)\n",
    "ax.set_ylabel(\"Discharge (mÂ³/s)\", font=font1)\n",
    "\n",
    "plt.xticks(date_list, date_list, fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.xlim(pd.Timestamp(1949, 1, 1), pd.Timestamp(2025, 1, 1))\n",
    "\n",
    "plt.title(f\"{river} River\", font=font1)\n",
    "\n",
    "ax.legend(fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_fig + f\"Discharge_Time_Series_{river}.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
