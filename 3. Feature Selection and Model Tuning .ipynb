{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = r\"Table\" + \"\\\\\"\n",
    "path_model = r\"Model\" + \"\\\\\"\n",
    "path_fig = r\"Figure\" + \"\\\\\"\n",
    "\n",
    "file_data = path_df + \"Data2_without_nan.csv\"\n",
    "\n",
    "N1 = 60  # Number of initial features\n",
    "N2 = 30  # Number of final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_data)\n",
    "# df = df.sample(frac=0.05, random_state=10)\n",
    "\n",
    "X = df.drop(columns=[\"River\", \"Station\", \"Date\", \"Discharge\"])\n",
    "y = df[\"Discharge\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(y_true, y_pred, decimal_digit=3, name=None):\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "    R = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    R2 = r2_score(y_true, y_pred)\n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    MAPE = (\n",
    "        np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    )  # Mean Absolute Percentage Error\n",
    "    try:\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    except ImportError:\n",
    "        from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "        RMSE = root_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    RRMSE = (RMSE / np.mean(y_true)) * 100\n",
    "    fRMSE = RMSE / np.std(y_true)\n",
    "    NRMSE = RMSE / (np.max(y_true) - np.min(y_true))\n",
    "    PBIAS = (np.sum(y_pred - y_true) / np.sum(y_true)) * 100\n",
    "    DDR = np.mean(\n",
    "        np.abs((y_pred - y_true) / np.mean(y_true))\n",
    "    )  # Developed Discrepancy Ratio (custom calculation)\n",
    "    IQR = np.percentile(y_true, 75) - np.percentile(y_true, 25)  # Inter-Quartile Range\n",
    "    RPIQ = IQR / RMSE  # Ratio of Performance to Inter-Quartile Range\n",
    "\n",
    "    # Calculate Kling-Gupta Efficiency\n",
    "    alpha = np.std(y_pred) / np.std(y_true)\n",
    "    beta = np.mean(y_pred) / np.mean(y_true)\n",
    "    KGE = 1 - np.sqrt((R - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "    # Store metrics\n",
    "    metrics = {\n",
    "        \"R\": R,\n",
    "        \"R2\": R2,\n",
    "        \"MAE\": MAE,\n",
    "        \"MAPE (%)\": MAPE,\n",
    "        \"RMSE\": RMSE,\n",
    "        \"RRMSE (%)\": RRMSE,\n",
    "        \"fRMSE\": fRMSE,\n",
    "        \"NRMSE\": NRMSE,\n",
    "        \"PBIAS (%)\": PBIAS,\n",
    "        \"DDR\": DDR,\n",
    "        \"RPIQ\": RPIQ,\n",
    "        \"KGE\": KGE,\n",
    "    }\n",
    "    metrics = pd.Series(metrics).round(decimal_digit)\n",
    "    if name:\n",
    "        metrics.name = name\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_best_model(metric):\n",
    "    N = metric.shape[0]\n",
    "\n",
    "    # Extract the score data for comparison\n",
    "    scores = metric.iloc[:, 3:].copy()\n",
    "\n",
    "    # Define metrics where higher is better and where lower is better\n",
    "    list1 = [\"R\", \"R2\", \"DDR\", \"RPIQ\", \"KGE\"]  # Higher is better\n",
    "    list2 = [\n",
    "        \"MAE\",\n",
    "        \"RMSE\",\n",
    "        \"RRMSE (%)\",\n",
    "        \"fRMSE\",\n",
    "        \"NRMSE\",\n",
    "        \"PBIAS (%)\",\n",
    "    ]  # Lower is better\n",
    "\n",
    "    # Initialize an empty DataFrame to store scores with points\n",
    "    scores_with_points = pd.DataFrame(index=scores.index)\n",
    "\n",
    "    # Use absolute values for PBIAS (%) for comparison\n",
    "    if \"PBIAS (%)\" in scores.columns:\n",
    "        scores[\"PBIAS (%)\"] = scores[\"PBIAS (%)\"].abs()\n",
    "\n",
    "    # Define a function to handle inf values\n",
    "    def apply_inf_handling(x, N, ascending):\n",
    "        # Check if there are any inf values\n",
    "        if np.isinf(x).any():\n",
    "            return pd.Series([0] * N)  # Assign zero points to all records\n",
    "        else:\n",
    "            # Calculate normal rankings\n",
    "            return x.rank(ascending=ascending, method=\"min\").apply(lambda r: N - r + 1)\n",
    "\n",
    "    # Calculate ranking scores for metrics where higher is better\n",
    "    if list1:\n",
    "        ranked_high_scores = scores[list1].apply(\n",
    "            lambda x: apply_inf_handling(x, N, ascending=False), axis=0\n",
    "        )\n",
    "        scores_with_points[list1] = ranked_high_scores\n",
    "\n",
    "    # Calculate ranking scores for metrics where lower is better\n",
    "    if list2:\n",
    "        ranked_low_scores = scores[list2].apply(\n",
    "            lambda x: apply_inf_handling(x, N, ascending=True), axis=0\n",
    "        )\n",
    "        scores_with_points[list2] = ranked_low_scores\n",
    "\n",
    "    # Calculate total score\n",
    "    metric[\"Total Score\"] = scores_with_points.sum(axis=1)\n",
    "\n",
    "    # Find the record with the highest total score\n",
    "    idx_max = metric[\"Total Score\"].idxmax()\n",
    "    best_model_row = metric.iloc[int(idx_max), :]\n",
    "\n",
    "    print(\"The best model is:\", best_model_row[\"Model\"])\n",
    "\n",
    "    # Save the information of the best model to a CSV file\n",
    "    metric.loc[idx_max, \"Best Model\"] = 1\n",
    "    metric.to_csv(path_df + \"Model_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial feature selection by feature importance ranking\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "feature_initial = np.argsort(feature_importances)[::-1][:N1]\n",
    "\n",
    "X_train1 = X_train.iloc[:, feature_initial]\n",
    "X_test1 = X_test.iloc[:, feature_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature selection by RFE\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rfe = RFE(estimator=rf, n_features_to_select=N2, step=1)\n",
    "rfe.fit(X_train1, y_train)\n",
    "\n",
    "ranking = rfe.ranking_\n",
    "ranking = pd.DataFrame(ranking, index=X_train1.columns, columns=[\"Rank\"]).sort_values(\n",
    "    by=\"Rank\", ascending=True\n",
    ")\n",
    "ranking.index.name = \"Feature\"\n",
    "\n",
    "features_selected = ranking[ranking[\"Rank\"] == 1]\n",
    "features_selected.to_csv(path_df + \"Selected_features_RF.csv\")\n",
    "\n",
    "X_train2 = X_train1[features_selected.index]\n",
    "X_test2 = X_test1[features_selected.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "}\n",
    "\n",
    "# Random search of parameters\n",
    "rf = RandomForestRegressor(random_state=42, bootstrap=True, n_jobs=-1)\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=120,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_random.fit(X_train2, y_train)\n",
    "\n",
    "# Get the best model\n",
    "model = rf_random.best_estimator_\n",
    "best_params = rf_random.best_params_\n",
    "\n",
    "# Save the model parameters\n",
    "with open(path_model + \"RF_best_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test2)\n",
    "df_pred = pd.DataFrame({\"Observed\": y_test, \"Predicted\": y_pred})\n",
    "df_pred.to_csv(path_df + \"RF_Prediction.csv\")\n",
    "\n",
    "m1 = calc_metric(y_test, y_pred, 3, \"RF\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.scatter(y_test, y_pred, s=2)\n",
    "\n",
    "min_value = min(min(y_test), min(y_pred))\n",
    "max_value = max(max(y_test), max(y_pred))\n",
    "ax.plot(\n",
    "    [min_value, max_value],\n",
    "    [min_value, max_value],\n",
    "    color=\"grey\",\n",
    "    linewidth=1,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"In-situ Discharge (m³/s)\")\n",
    "ax.set_ylabel(\"RF Predicted Discharge (m³/s)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(path_fig + \"Scatter_RF.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial feature selection by feature importance ranking\n",
    "model = XGBRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "feature_initial = np.argsort(feature_importances)[::-1][:N1]\n",
    "\n",
    "X_train1 = X_train.iloc[:, feature_initial]\n",
    "X_test1 = X_test.iloc[:, feature_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature selection by RFE\n",
    "xgb = XGBRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rfe = RFE(estimator=xgb, n_features_to_select=N2, step=1)\n",
    "rfe.fit(X_train1, y_train)\n",
    "\n",
    "ranking = rfe.ranking_\n",
    "ranking = pd.DataFrame(ranking, index=X_train1.columns, columns=[\"Rank\"]).sort_values(\n",
    "    by=\"Rank\", ascending=True\n",
    ")\n",
    "ranking.index.name = \"Feature\"\n",
    "\n",
    "features_selected = ranking[ranking[\"Rank\"] == 1]\n",
    "features_selected.to_csv(path_df + \"Selected_features_XGB.csv\")\n",
    "\n",
    "X_train2 = X_train1[features_selected.index]\n",
    "X_test2 = X_test1[features_selected.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"subsample\": [0.5, 0.7, 1],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1],\n",
    "}\n",
    "\n",
    "# Random search of parameters\n",
    "xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "xgb_random.fit(X_train2, y_train)\n",
    "\n",
    "# Get the best model\n",
    "model = xgb_random.best_estimator_\n",
    "best_params = xgb_random.best_params_\n",
    "\n",
    "# Save the model parameters\n",
    "with open(path_model + \"XGB_best_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test2)\n",
    "df_pred = pd.DataFrame({\"Observed\": y_test, \"Predicted\": y_pred})\n",
    "df_pred.to_csv(path_df + \"XGB_Prediction.csv\")\n",
    "\n",
    "m2 = calc_metric(y_test, y_pred, 3, \"XGB\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.scatter(y_test, y_pred, s=2)\n",
    "\n",
    "min_value = min(min(y_test), min(y_pred))\n",
    "max_value = max(max(y_test), max(y_pred))\n",
    "ax.plot(\n",
    "    [min_value, max_value],\n",
    "    [min_value, max_value],\n",
    "    color=\"grey\",\n",
    "    linewidth=1,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"In-situ Discharge (m³/s)\")\n",
    "ax.set_ylabel(\"XGB Predicted Discharge (m³/s)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(path_fig + \"Scatter_XGB.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.concat([m1, m2], axis=1).T\n",
    "metrics.index.name = \"Model\"\n",
    "metrics.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river = df[\"River\"].iloc[0]\n",
    "station = df[\"Station\"].iloc[0]\n",
    "\n",
    "metrics.insert(0, \"River\", river)\n",
    "metrics.insert(1, \"Station\", station)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_model(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
